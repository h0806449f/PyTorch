{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XOxJz4aeujSB",
        "VJCADvpo1cUR",
        "EQYMQ8ut1hD7",
        "4pnOJYw0uqKB",
        "9Yt1GRQrwRIA",
        "8WypbPcN6vWx",
        "aSjxInz1_zEH",
        "JI5qH2_lPfWI",
        "FnIvLpK2Pi-K"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM5nNIOaua6Gza19B3tEjPl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h0806449f/PyTorch/blob/main/LightningAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **== Setups ==**"
      ],
      "metadata": {
        "id": "XOxJz4aeujSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import"
      ],
      "metadata": {
        "id": "VJCADvpo1cUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7-mzkfWz8WH",
        "outputId": "66c996b6-a84b-45c4-e1a0-6d22c1ace51a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gitpython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6FdX5QHFsZr",
        "outputId": "c114d92e-89cb-4c29-ab2b-97492c7218ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gitpython\n",
            "  Downloading GitPython-3.1.32-py3-none-any.whl (188 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1 (from gitpython)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, gitdb, gitpython\n",
            "Successfully installed gitdb-4.0.10 gitpython-3.1.32 smmap-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdQdycnwUsqw",
        "outputId": "dc4fb981-39b3-49b0-e9d5-77951749fd9a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.0.5-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Jinja2<5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (3.1.2)\n",
            "Requirement already satisfied: PyYAML<8.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0)\n",
            "Collecting arrow<3.0,>=1.2.0 (from lightning)\n",
            "  Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting backoff<4.0,>=2.2.1 (from lightning)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: beautifulsoup4<6.0,>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.11.2)\n",
            "Requirement already satisfied: click<10.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (8.1.4)\n",
            "Collecting croniter<1.5.0,>=1.3.0 (from lightning)\n",
            "  Downloading croniter-1.4.1-py2.py3-none-any.whl (19 kB)\n",
            "Collecting dateutils<2.0 (from lightning)\n",
            "  Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n",
            "Collecting deepdiff<8.0,>=5.7.0 (from lightning)\n",
            "  Downloading deepdiff-6.3.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.7/70.7 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi<2.0,>=0.92.0 (from lightning)\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Collecting inquirer<5.0,>=2.10.0 (from lightning)\n",
            "  Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
            "Collecting lightning-cloud>=0.5.37 (from lightning)\n",
            "  Downloading lightning_cloud-0.5.37-py3-none-any.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.7/596.7 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lightning-utilities<2.0,>=0.7.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lightning) (23.1)\n",
            "Requirement already satisfied: psutil<7.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (5.9.5)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.10.11)\n",
            "Collecting python-multipart<2.0,>=0.0.5 (from lightning)\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.27.1)\n",
            "Requirement already satisfied: rich<15.0,>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (13.4.2)\n",
            "Collecting starlette (from lightning)\n",
            "  Downloading starlette-0.30.0-py3-none-any.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starsessions<2.0,>=1.2.1 (from lightning)\n",
            "  Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch<4.0,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.0.1+cu118)\n",
            "Collecting torchmetrics<2.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.0.1-py3-none-any.whl (729 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m729.2/729.2 kB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.65.0)\n",
            "Requirement already satisfied: traitlets<7.0,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (5.7.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.7.1)\n",
            "Requirement already satisfied: urllib3<4.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.26.16)\n",
            "Collecting uvicorn<2.0 (from lightning)\n",
            "  Downloading uvicorn-0.23.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<3.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.6.1)\n",
            "Collecting websockets<13.0 (from lightning)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.0.5-py3-none-any.whl (722 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m722.4/722.4 kB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow<3.0,>=1.2.0->lightning) (2.8.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<6.0,>=4.8.0->lightning) (2.4.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from dateutils<2.0->lightning) (2022.7.1)\n",
            "Collecting ordered-set<4.2.0,>=4.0.2 (from deepdiff<8.0,>=5.7.0->lightning)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Collecting starlette (from lightning)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec<2025.0,>=2022.5.0->lightning) (3.8.4)\n",
            "Collecting blessed>=1.19.0 (from inquirer<5.0,>=2.10.0->lightning)\n",
            "  Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-editor>=1.0.4 (from inquirer<5.0,>=2.10.0->lightning)\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Collecting readchar>=3.0.6 (from inquirer<5.0,>=2.10.0->lightning)\n",
            "  Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<5.0->lightning) (2.1.3)\n",
            "Collecting pyjwt (from lightning-cloud>=0.5.37->lightning)\n",
            "  Downloading PyJWT-2.7.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from lightning-cloud>=0.5.37->lightning) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<4.0->lightning) (3.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<15.0,>=12.3.0->lightning) (2.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette->lightning) (3.7.1)\n",
            "Requirement already satisfied: itsdangerous<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from starsessions<2.0,>=1.2.1->lightning) (2.1.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.11.0->lightning) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<4.0,>=1.11.0->lightning) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch<4.0,>=1.11.0->lightning) (16.0.6)\n",
            "Collecting h11>=0.8 (from uvicorn<2.0->lightning)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec<2025.0,>=2022.5.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette->lightning) (1.1.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning) (0.2.6)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<15.0,>=12.3.0->lightning) (0.1.2)\n",
            "Requirement already satisfied: setuptools>=41.0 in /usr/local/lib/python3.10/dist-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.11.0->lightning) (1.3.0)\n",
            "Installing collected packages: python-editor, websockets, readchar, python-multipart, pyjwt, ordered-set, lightning-utilities, h11, blessed, backoff, uvicorn, starlette, inquirer, deepdiff, dateutils, croniter, arrow, starsessions, fastapi, lightning-cloud, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed arrow-1.2.3 backoff-2.2.1 blessed-1.20.0 croniter-1.4.1 dateutils-0.6.12 deepdiff-6.3.1 fastapi-0.100.0 h11-0.14.0 inquirer-3.1.3 lightning-2.0.5 lightning-cloud-0.5.37 lightning-utilities-0.9.0 ordered-set-4.1.0 pyjwt-2.7.0 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.5 readchar-4.0.5 starlette-0.27.0 starsessions-1.3.0 torchmetrics-1.0.1 uvicorn-0.23.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from git import Repo\n",
        "from PIL import Image\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "TWD9iZc_w5nf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.dataset import random_split\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from torchinfo import summary\n",
        "\n",
        "from lightning import LightningModule\n",
        "from lightning import Trainer"
      ],
      "metadata": {
        "id": "FnPayfqajHkm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## functions"
      ],
      "metadata": {
        "id": "EQYMQ8ut1hD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, loss_fn, optimizer, device):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for batch, (X_train, y_train) in enumerate(dataloader):\n",
        "        X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "\n",
        "        train_pred = model(X_train)\n",
        "\n",
        "        loss = loss_fn(train_pred, y_train)\n",
        "        train_loss = train_loss + loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_pred_label = torch.argmax(torch.softmax(train_pred, dim = 1), dim = 1)\n",
        "        train_acc = train_acc + (train_pred_label == y_train).sum().item() / len(train_pred)\n",
        "\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "\n",
        "    return train_loss, train_acc"
      ],
      "metadata": {
        "id": "bajyKhXo1lwn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, dataloader, loss_fn, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch, (X_test, y_test) in enumerate(dataloader):\n",
        "            X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "            test_pred = model(X_test)\n",
        "\n",
        "            loss = loss_fn(test_pred, y_test)\n",
        "            test_loss = test_loss + loss.item()\n",
        "\n",
        "            test_pred_label = torch.argmax(torch.softmax(test_pred, dim = 1), dim = 1)\n",
        "            test_acc = test_acc + (test_pred_label == y_test).sum().item() / len(test_pred)\n",
        "\n",
        "        test_loss = test_loss / len(dataloader)\n",
        "        test_acc = test_acc / len(dataloader)\n",
        "\n",
        "        return test_loss, test_acc"
      ],
      "metadata": {
        "id": "Yjbn7Kpk22Hr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def val(model, dataloader, loss_fn, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    val_loss, val_acc = 0, 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for batch, (X_val, y_val) in enumerate(dataloader):\n",
        "            X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "\n",
        "            val_pred = model(X_val)\n",
        "\n",
        "            loss = loss_fn(val_pred, y_val)\n",
        "            val_loss = val_loss + loss.item()\n",
        "\n",
        "            val_pred_label = torch.argmax(torch.softmax(val_pred, dim = 1), dim = 1)\n",
        "            val_acc = val_acc + (val_pred_label == y_val).sum().item() / len(val_pred)\n",
        "\n",
        "        val_loss = val_loss / len(dataloader)\n",
        "        val_acc = val_acc / len(dataloader)\n",
        "\n",
        "        return val_loss, val_acc"
      ],
      "metadata": {
        "id": "UvE1zdGh4LtA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_loop(model, train_dataloader, test_dataloader, loss_fn, optimizer, epochs, device):\n",
        "    results = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, train_acc = train(model = model,\n",
        "                                      dataloader = train_dataloader,\n",
        "                                      loss_fn = loss_fn,\n",
        "                                      optimizer = optimizer,\n",
        "                                      device = device)\n",
        "\n",
        "        val_loss, val_acc = val(model = model,\n",
        "                                dataloader = val_dataloader,\n",
        "                                loss_fn = loss_fn,\n",
        "                                device = device)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}\\n\"\n",
        "              f\"Train loss: {train_loss:.4f} | Train acc: {(train_acc*100):.2f}%\\n\"\n",
        "              f\"Val loss: {val_loss:.4f} | Val acc: {(val_acc*100):.2f}%\"\n",
        "              )\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"val_loss\"].append(val_loss)\n",
        "        results[\"val_acc\"].append(val_acc)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "PJhFV6sa4vcg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **== MNIST ==**"
      ],
      "metadata": {
        "id": "4pnOJYw0uqKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process"
      ],
      "metadata": {
        "id": "9Yt1GRQrwRIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "transform\n"
      ],
      "metadata": {
        "id": "oETEcR_iuWWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = 0.5, std = 0.5)\n",
        "])"
      ],
      "metadata": {
        "id": "I4o7VX6Wr68Z"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "sBBBb3uewJDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST(\n",
        "    root = \"./data/mnist\",\n",
        "    train = True,\n",
        "    transform = transform,\n",
        "    download = True)\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    root = \"./data/mnist\",\n",
        "    train = False,\n",
        "    transform = transform,\n",
        "    download = True)"
      ],
      "metadata": {
        "id": "_RtX4zxYtSvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09f0418a-6011-4325-f3ab-16077d2587ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 278827644.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 102919026.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 93091914.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/mnist/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 22203413.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/mnist/MNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "split dataset"
      ],
      "metadata": {
        "id": "rOxzaNaawME6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset = random_split(train_dataset,\n",
        "                                          lengths=[50000, 10000])"
      ],
      "metadata": {
        "id": "aQQu3Km9u41F"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset), len(val_dataset), len(test_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzgbvPTDvhd6",
        "outputId": "d6bc542c-36ce-4fe1-f242-a38aeb96ef76"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataloader"
      ],
      "metadata": {
        "id": "UV_Sv5GFwOdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset,\n",
        "                              batch_size = 32,\n",
        "                              shuffle = True,\n",
        "                              drop_last = True)\n",
        "\n",
        "val_dataloader = DataLoader(val_dataset,\n",
        "                            batch_size = 32,\n",
        "                            shuffle = False,\n",
        "                            drop_last = True)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset,\n",
        "                             batch_size = 32,\n",
        "                             shuffle = False,\n",
        "                             drop_last = True)"
      ],
      "metadata": {
        "id": "coTde45fv9qc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check dataloader distribution"
      ],
      "metadata": {
        "id": "jCwZH9NAxe8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_counter = Counter()\n",
        "\n",
        "# for imgs, labels in train_dataloader:\n",
        "#     train_counter.update(labels.tolist())\n",
        "\n",
        "# print(\"Training label distribution:\")\n",
        "# print(sorted(train_counter.items()))"
      ],
      "metadata": {
        "id": "W4ZU0K6zwk8U"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f\"{imgs.shape} -> batch_szie, channel, height, width\")"
      ],
      "metadata": {
        "id": "Gb4yiTW7xngH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST_model_1(torch.nn.Module):\n",
        "    def __init__(self, num_features, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(num_features, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p = 0.2),\n",
        "            nn.Linear(128, 32),\n",
        "            nn.BatchNorm1d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p = 0.2),\n",
        "            nn.Linear(32, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flat(x)\n",
        "        x = self.layers(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "MNIST_model_1 = MNIST_model_1(num_features = 1*28*28, num_classes = 10)"
      ],
      "metadata": {
        "id": "6IyawLVxzjdv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and results"
      ],
      "metadata": {
        "id": "8WypbPcN6vWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(MNIST_model_1.parameters(),\n",
        "                             lr = 0.001)\n",
        "\n",
        "MNIST_model_1_results = train_val_loop(model = MNIST_model_1,\n",
        "                                       train_dataloader = train_dataloader,\n",
        "                                       test_dataloader = val_dataloader,\n",
        "                                       loss_fn = loss_fn,\n",
        "                                       optimizer = optimizer,\n",
        "                                       epochs = 10,\n",
        "                                       device = device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luexOJj81U3A",
        "outputId": "af5d5c23-7d92-4b45-ef14-99fe1f2cb6c7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train loss: 0.4524 | Train acc: 88.44%\n",
            "Val loss: 0.1516 | Val acc: 95.41%\n",
            "Epoch: 2\n",
            "Train loss: 0.2254 | Train acc: 93.39%\n",
            "Val loss: 0.1102 | Val acc: 96.69%\n",
            "Epoch: 3\n",
            "Train loss: 0.1902 | Train acc: 94.38%\n",
            "Val loss: 0.0929 | Val acc: 97.11%\n",
            "Epoch: 4\n",
            "Train loss: 0.1654 | Train acc: 95.10%\n",
            "Val loss: 0.0882 | Val acc: 97.31%\n",
            "Epoch: 5\n",
            "Train loss: 0.1543 | Train acc: 95.33%\n",
            "Val loss: 0.0822 | Val acc: 97.59%\n",
            "Epoch: 6\n",
            "Train loss: 0.1378 | Train acc: 95.80%\n",
            "Val loss: 0.0808 | Val acc: 97.60%\n",
            "Epoch: 7\n",
            "Train loss: 0.1321 | Train acc: 96.01%\n",
            "Val loss: 0.0766 | Val acc: 97.82%\n",
            "Epoch: 8\n",
            "Train loss: 0.1269 | Train acc: 96.12%\n",
            "Val loss: 0.0776 | Val acc: 97.63%\n",
            "Epoch: 9\n",
            "Train loss: 0.1201 | Train acc: 96.35%\n",
            "Val loss: 0.0736 | Val acc: 97.84%\n",
            "Epoch: 10\n",
            "Train loss: 0.1150 | Train acc: 96.49%\n",
            "Val loss: 0.0717 | Val acc: 97.91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_1, test_acc_1 = test(model = MNIST_model_1,\n",
        "                           dataloader = test_dataloader,\n",
        "                           loss_fn = loss_fn,\n",
        "                           device = device)\n",
        "print(f\"Test loss: {test_loss_1:.4f} | Test acc: {(test_acc_1*100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y2mTOWh_J24",
        "outputId": "68acbef8-8520-4d70-b0fd-68efee1cc7e4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.0716 | Test acc: 97.75%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **== Custom dataset, dataloader ==**"
      ],
      "metadata": {
        "id": "aSjxInz1_zEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "function"
      ],
      "metadata": {
        "id": "7xfzQwkCOLnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class CustomDataset(Dataset):\n",
        "#     # Set up attributes\n",
        "#     def __init__(self, csv_path, img_dir, transform = None):\n",
        "#         df = pd.read_csv(csv_path)\n",
        "#         self.img_dir = img_dir\n",
        "#         self.transform = transform\n",
        "\n",
        "#         self.img_names = df[\"filepath\"]\n",
        "#         self.labels = df[\"label\"]\n",
        "\n",
        "#     # Define how to get single record\n",
        "#     def __getitem__(self, index):\n",
        "#         img = Image.open(os.path.join(self.img_dir, self.img_names[index]))\n",
        "\n",
        "#         if self.transform is not None:\n",
        "#             img = self.transform(img)\n",
        "\n",
        "#         label = self.labels[index]\n",
        "\n",
        "#         return img, label\n",
        "\n",
        "#     # Return the length of dataset\n",
        "#     def __len__(self):\n",
        "#         return self.labels.shape[0]"
      ],
      "metadata": {
        "id": "Y_SGtM4-BJ8P"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "raw dara -> pandas df"
      ],
      "metadata": {
        "id": "EKXgqP8hONig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Download\n",
        "# if not os.path.exists(\"mnist-pngs\"):\n",
        "#     Repo.clone_from(\"https://github.com/rasbt/mnist-pngs\", \"mnist-pngs\")\n",
        "\n",
        "# # Read csv\n",
        "# df_train = pd.read_csv(\"mnist-pngs/train.csv\")\n",
        "# df_test = pd.read_csv(\"mnist-pngs/test.csv\")\n",
        "\n",
        "# # 順序打亂 (為了使val dataset 也包含多樣資料)\n",
        "# df_train = df_train.sample(frac = 1, random_state = 123)\n",
        "\n",
        "# # Split train & val dataset\n",
        "# split_index = round(df_train.shape[0] * 0.9)\n",
        "\n",
        "# df_new_train = df_train.iloc[:split_index]\n",
        "# df_new_val = df_train.iloc[split_index:]\n",
        "\n",
        "# df_new_train.to_csv(\"mnist-pngs/new_train.csv\", index=None)\n",
        "# df_new_val.to_csv(\"mnist-pngs/new_val.csv\", index=None)"
      ],
      "metadata": {
        "id": "gZgufRGRF2IZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transform"
      ],
      "metadata": {
        "id": "LSA0ACAIOSIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_transforms = {\n",
        "#     \"train\": transforms.Compose([\n",
        "#         transforms.Resize(32),\n",
        "#         transforms.RandomCrop((28, 28)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean = 0.5, std = 0.5)\n",
        "#     ]),\n",
        "#     \"test\": transforms.Compose([\n",
        "#         transforms.Resize(32),\n",
        "#         transforms.CenterCrop((28, 28)),\n",
        "#         transforms.ToTensor(),\n",
        "#         transforms.Normalize(mean = 0.5, std = 0.5)\n",
        "#     ])\n",
        "# }"
      ],
      "metadata": {
        "id": "WoNPALl9MXKH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset -> dataloader"
      ],
      "metadata": {
        "id": "AiQRyJTxOUD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset = CustomDataset(\n",
        "#     csv_path = \"mnist-pngs/new_train.csv\",\n",
        "#     img_dir = \"mnist-pngs/\",\n",
        "#     transform = data_transforms[\"train\"])\n",
        "\n",
        "# train_dataloader = DataLoader(\n",
        "#     dataset = train_dataset,\n",
        "#     batch_size = 32,\n",
        "#     shuffle = True)"
      ],
      "metadata": {
        "id": "KPCEoxk4NXF6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **== CNN -> regression dataset ==**"
      ],
      "metadata": {
        "id": "JI5qH2_lPfWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normal CNN\n",
        "\"\"\"\n",
        "    input\n",
        "    hidden layers\n",
        "    softmax\n",
        "    argmax\n",
        "    output layers: number of class names\n",
        "\"\"\"\n",
        "\n",
        "# regression\n",
        "\"\"\"\n",
        "    input\n",
        "    hidden layers\n",
        "    output layers: 1\n",
        "    loss_fn -> Meas Squared Error -> nn.MSELoss()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jrx2QWksQYyr",
        "outputId": "0b66a858-43b9-498a-d747-9de3c8e53699"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    input\\n    hidden layers\\n    output layers: 1\\n    loss_fn -> Meas Squared Error -> nn.MSELoss()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **== CNN -> CIFAR10 ==**\n",
        "transfer learning -> fine-tune"
      ],
      "metadata": {
        "id": "FnIvLpK2Pi-K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process"
      ],
      "metadata": {
        "id": "7eaCmhyj68ZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "transforms"
      ],
      "metadata": {
        "id": "fSpAnboLbs-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
        "\n",
        "transform = weights.transforms()"
      ],
      "metadata": {
        "id": "cSQdqiRtbegE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "_OdnNV4ubzCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torchvision.datasets.CIFAR10(root = \"./data\",\n",
        "                                             train = True,\n",
        "                                             download = True,\n",
        "                                             transform = transform)\n",
        "\n",
        "val_dataset = torchvision.datasets.CIFAR10(root = \"./data\",\n",
        "                                             train = False,\n",
        "                                             download = True,\n",
        "                                             transform = transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = \"./data\",\n",
        "                                            train = False,\n",
        "                                            download = True,\n",
        "                                            transform = transform)"
      ],
      "metadata": {
        "id": "vemIZbrObvQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataloader"
      ],
      "metadata": {
        "id": "cBwvZL85craS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "\n",
        "train_dataloader = DataLoader(dataset = train_dataset,\n",
        "                              batch_size = BATCH_SIZE,\n",
        "                              shuffle = True,\n",
        "                              drop_last = True)\n",
        "\n",
        "val_dataloader = DataLoader(dataset = val_dataset,\n",
        "                            batch_size = BATCH_SIZE,\n",
        "                            shuffle = False,\n",
        "                            drop_last = False)\n",
        "\n",
        "test_dataloader = DataLoader(dataset = test_dataset,\n",
        "                             batch_size = BATCH_SIZE,\n",
        "                             shuffle = False,\n",
        "                             drop_last = False)"
      ],
      "metadata": {
        "id": "i0v2r-fNcsma"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "A9j7pK86d7dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.ResNet18_Weights.DEFAULT\n",
        "\n",
        "CIFAR10_model1 = torchvision.models.resnet18(weights = weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q2wtocyaqc5",
        "outputId": "f4017ce4-f834-44c0-a0b1-30d964d7822b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 76.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in CIFAR10_model1.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "zXkrbu5Ld50H"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CIFAR10_model1_classifier = nn.Sequential(\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p = 0.3),\n",
        "    nn.Linear(1000, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p = 0.2),\n",
        "    nn.Linear(128, 10)\n",
        "\n",
        ")\n",
        "\n",
        "CIFAR10_model1 = nn.Sequential(\n",
        "    CIFAR10_model1,\n",
        "    CIFAR10_model1_classifier\n",
        ")"
      ],
      "metadata": {
        "id": "WERvB9EAhU5B"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(CIFAR10_model1,\n",
        "        input_size = (1, 3, 32, 32),\n",
        "        col_names = [\"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width = 17)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe-zs_X_dfLn",
        "outputId": "ed72dfad-ae35-4b31-db51-319cb840703c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "================================================================================================\n",
              "Layer (type:depth-idx)                        Output Shape      Param #           Trainable\n",
              "================================================================================================\n",
              "Sequential                                    [1, 10]           --                Partial\n",
              "├─ResNet: 1-1                                 [1, 1000]         --                False\n",
              "│    └─Conv2d: 2-1                            [1, 64, 16, 16]   (9,408)           False\n",
              "│    └─BatchNorm2d: 2-2                       [1, 64, 16, 16]   (128)             False\n",
              "│    └─ReLU: 2-3                              [1, 64, 16, 16]   --                --\n",
              "│    └─MaxPool2d: 2-4                         [1, 64, 8, 8]     --                --\n",
              "│    └─Sequential: 2-5                        [1, 64, 8, 8]     --                False\n",
              "│    │    └─BasicBlock: 3-1                   [1, 64, 8, 8]     (73,984)          False\n",
              "│    │    └─BasicBlock: 3-2                   [1, 64, 8, 8]     (73,984)          False\n",
              "│    └─Sequential: 2-6                        [1, 128, 4, 4]    --                False\n",
              "│    │    └─BasicBlock: 3-3                   [1, 128, 4, 4]    (230,144)         False\n",
              "│    │    └─BasicBlock: 3-4                   [1, 128, 4, 4]    (295,424)         False\n",
              "│    └─Sequential: 2-7                        [1, 256, 2, 2]    --                False\n",
              "│    │    └─BasicBlock: 3-5                   [1, 256, 2, 2]    (919,040)         False\n",
              "│    │    └─BasicBlock: 3-6                   [1, 256, 2, 2]    (1,180,672)       False\n",
              "│    └─Sequential: 2-8                        [1, 512, 1, 1]    --                False\n",
              "│    │    └─BasicBlock: 3-7                   [1, 512, 1, 1]    (3,673,088)       False\n",
              "│    │    └─BasicBlock: 3-8                   [1, 512, 1, 1]    (4,720,640)       False\n",
              "│    └─AdaptiveAvgPool2d: 2-9                 [1, 512, 1, 1]    --                --\n",
              "│    └─Linear: 2-10                           [1, 1000]         (513,000)         False\n",
              "├─Sequential: 1-2                             [1, 10]           --                True\n",
              "│    └─ReLU: 2-11                             [1, 1000]         --                --\n",
              "│    └─Dropout: 2-12                          [1, 1000]         --                --\n",
              "│    └─Linear: 2-13                           [1, 128]          128,128           True\n",
              "│    └─ReLU: 2-14                             [1, 128]          --                --\n",
              "│    └─Dropout: 2-15                          [1, 128]          --                --\n",
              "│    └─Linear: 2-16                           [1, 10]           1,290             True\n",
              "================================================================================================\n",
              "Total params: 11,818,930\n",
              "Trainable params: 129,418\n",
              "Non-trainable params: 11,689,512\n",
              "Total mult-adds (M): 37.66\n",
              "================================================================================================\n",
              "Input size (MB): 0.01\n",
              "Forward/backward pass size (MB): 0.82\n",
              "Params size (MB): 47.28\n",
              "Estimated Total Size (MB): 48.11\n",
              "================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and results"
      ],
      "metadata": {
        "id": "8KNJMAN16_6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(CIFAR10_model1.parameters(),\n",
        "                             lr = 0.001)"
      ],
      "metadata": {
        "id": "DQHjl42YeMSK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_CIFAR10_model1 = train_val_loop(model = CIFAR10_model1,\n",
        "                                        train_dataloader = train_dataloader,\n",
        "                                        test_dataloader = val_dataloader,\n",
        "                                        loss_fn = loss_fn,\n",
        "                                        optimizer = optimizer,\n",
        "                                        epochs = 10,\n",
        "                                        device = device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-HbGTXlee6p",
        "outputId": "b1411c62-7343-4824-bf87-5be1b4d621e4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train loss: 1.2090 | Train acc: 58.58%\n",
            "Val loss: 0.8610 | Val acc: 70.83%\n",
            "Epoch: 2\n",
            "Train loss: 1.1359 | Train acc: 61.45%\n",
            "Val loss: 0.8312 | Val acc: 70.91%\n",
            "Epoch: 3\n",
            "Train loss: 1.1140 | Train acc: 62.11%\n",
            "Val loss: 0.8156 | Val acc: 71.97%\n",
            "Epoch: 4\n",
            "Train loss: 1.1019 | Train acc: 62.79%\n",
            "Val loss: 0.8011 | Val acc: 72.83%\n",
            "Epoch: 5\n",
            "Train loss: 1.1004 | Train acc: 62.66%\n",
            "Val loss: 0.7840 | Val acc: 73.64%\n",
            "Epoch: 6\n",
            "Train loss: 1.0945 | Train acc: 63.05%\n",
            "Val loss: 0.8004 | Val acc: 73.01%\n",
            "Epoch: 7\n",
            "Train loss: 1.0875 | Train acc: 63.32%\n",
            "Val loss: 0.7955 | Val acc: 73.16%\n",
            "Epoch: 8\n",
            "Train loss: 1.0867 | Train acc: 63.56%\n",
            "Val loss: 0.7809 | Val acc: 73.67%\n",
            "Epoch: 9\n",
            "Train loss: 1.0892 | Train acc: 63.50%\n",
            "Val loss: 0.7978 | Val acc: 73.54%\n",
            "Epoch: 10\n",
            "Train loss: 1.0787 | Train acc: 63.96%\n",
            "Val loss: 0.7842 | Val acc: 73.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_2, test_acc_2 = test(model = CIFAR10_model1,\n",
        "                               dataloader = test_dataloader,\n",
        "                               loss_fn = loss_fn,\n",
        "                               device = device)\n",
        "\n",
        "print(f\"Test loss: {test_loss_2:.4f} | Test acc: {(test_acc_2*100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFx8MmWf6bXK",
        "outputId": "3b17f29e-059b-4c86-dff8-f826d8453e8a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.7842 | Test acc: 73.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save, load, predict"
      ],
      "metadata": {
        "id": "L8dvEpxV0NjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "save model"
      ],
      "metadata": {
        "id": "lvM5vAp8thBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(CIFAR10_model1, \"./data/mode_1forCIFAR10.pt\")"
      ],
      "metadata": {
        "id": "Av6hJ0Qyth9D"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load model"
      ],
      "metadata": {
        "id": "CXSdua6U0KPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load(\"/content/data/mode_1forCIFAR10.pt\")"
      ],
      "metadata": {
        "id": "Kar-_yUg0GgR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "predict"
      ],
      "metadata": {
        "id": "gI_4Hrx50Lex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.eval()\n",
        "\n",
        "image, label = next(iter(test_dataset))\n",
        "image = image.unsqueeze(dim = 0).to(device)\n",
        "\n",
        "logits = model(image)\n",
        "pred_probality = torch.softmax(logits, dim = 1)\n",
        "pred_label = torch.argmax(pred_probality, dim = 1).item()\n",
        "\n",
        "print(f\"Predict label: {pred_label} | Actual label: {label}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWjblXtAuAOv",
        "outputId": "14daf845-ce47-4db3-93d7-ae8e7950d977"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predict label: 3 | Actual label: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune\n",
        "pretrained-model -> only train classifier -> results  \n",
        "unfreeze -> train model again with less epochs"
      ],
      "metadata": {
        "id": "s5URzv_A6ig1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in CIFAR10_model1.parameters():\n",
        "    param.requires_grad = True"
      ],
      "metadata": {
        "id": "1SMdRyNB6hZL"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v2_results_CIFAR10_model1 = train_val_loop(model = CIFAR10_model1,\n",
        "                                           train_dataloader = train_dataloader,\n",
        "                                           test_dataloader = val_dataloader,\n",
        "                                           loss_fn = loss_fn,\n",
        "                                           optimizer = optimizer,\n",
        "                                           epochs = 10,\n",
        "                                           device = device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd4A4LEs6yOS",
        "outputId": "54d6af2c-5277-4610-a894-93d8ebe26747"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "Train loss: 2.1647 | Train acc: 15.48%\n",
            "Val loss: 1.9148 | Val acc: 24.77%\n",
            "Epoch: 2\n",
            "Train loss: 1.9322 | Train acc: 25.86%\n",
            "Val loss: 1.6815 | Val acc: 37.07%\n",
            "Epoch: 3\n",
            "Train loss: 1.6291 | Train acc: 40.08%\n",
            "Val loss: 1.4319 | Val acc: 47.02%\n",
            "Epoch: 4\n",
            "Train loss: 1.4417 | Train acc: 48.24%\n",
            "Val loss: 1.2128 | Val acc: 57.53%\n",
            "Epoch: 5\n",
            "Train loss: 1.2883 | Train acc: 54.33%\n",
            "Val loss: 1.0194 | Val acc: 64.14%\n",
            "Epoch: 6\n",
            "Train loss: 1.1542 | Train acc: 59.48%\n",
            "Val loss: 0.9788 | Val acc: 67.30%\n",
            "Epoch: 7\n",
            "Train loss: 1.0292 | Train acc: 63.90%\n",
            "Val loss: 0.8969 | Val acc: 70.02%\n",
            "Epoch: 8\n",
            "Train loss: 0.9067 | Train acc: 68.46%\n",
            "Val loss: 0.7782 | Val acc: 73.47%\n",
            "Epoch: 9\n",
            "Train loss: 0.8283 | Train acc: 71.10%\n",
            "Val loss: 0.8823 | Val acc: 70.71%\n",
            "Epoch: 10\n",
            "Train loss: 0.8119 | Train acc: 72.55%\n",
            "Val loss: 0.8662 | Val acc: 70.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss_3, test_acc_3 = test(model = CIFAR10_model1,\n",
        "                               dataloader = test_dataloader,\n",
        "                               loss_fn = loss_fn,\n",
        "                               device = device)\n",
        "\n",
        "print(f\"Test loss: {test_loss_3:.4f} | Test acc: {(test_acc_3*100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDJncj3G7rro",
        "outputId": "77cc458e-6415-4441-fc49-1b93cc3ba210"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.8662 | Test acc: 70.44%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **== TEMP ==**"
      ],
      "metadata": {
        "id": "agAZmgCwPjxe"
      }
    }
  ]
}