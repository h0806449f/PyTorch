{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D-LpNa7WnO8P",
        "sXngvY3RB2m6",
        "IUGEv06fB9lX",
        "pzMGTvmyCOKC",
        "JxDTqvVvGd89",
        "yjRFnLYvGztz",
        "rQKchKarHAhy",
        "tAAxpCwvI34V",
        "QvJ38UlbJUQ7",
        "zfHgsB0PMxNj",
        "1OIh41LDPyT-",
        "q6kMjU9VM2M4",
        "ajJf7RnOTd3G"
      ],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1MCTK8bQ1MoXcXoKyC5at",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/h0806449f/PyTorch/blob/main/NLP_first_try.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **== 0. 簡介: transformer可以做什麼 ==**\n",
        "from HuggingFace"
      ],
      "metadata": {
        "id": "D-LpNa7WnO8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]\n",
        "\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "yQOt9BfAjggT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8d723ec-624e-4740-ba55-5102b5a980cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.13.1)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.12.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.3.1)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (0.1.99)\n",
            "Requirement already satisfied: protobuf<=3.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]) (3.20.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 情緒分析\n",
        "classifier = pipeline(model = \"distilbert-base-uncased-finetuned-sst-2-english\", # Dfault model\n",
        "                      task = \"sentiment-analysis\")\n",
        "\n",
        "\n",
        "classifier(\"首次嘗試使用NLP相關模型, 模型來自於HuggingFace, 看起來有點厲害\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1rgzdfTj4FH",
        "outputId": "471b3dc1-ba28-46e3-9993-56cb7377238d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'NEGATIVE', 'score': 0.970554769039154}]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 零樣本 - 文本分類\n",
        "classifier = pipeline(model = \"facebook/bart-large-mnli\", # Default model\n",
        "                      task = \"zero-shot-classification\")\n",
        "\n",
        "classifier(\"This is a course about the Transformers library\",\n",
        "           candidate_labels=[\"education\", \"politics\", \"business\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWEfGIA8l_LL",
        "outputId": "9e599e18-602c-4c57-8d87-0361eacfcca5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sequence': 'This is a course about the Transformers library',\n",
              " 'labels': ['education', 'business', 'politics'],\n",
              " 'scores': [0.8445989489555359, 0.11197412759065628, 0.04342695698142052]}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 文本生成\n",
        "generator = pipeline(model = \"gpt2\", # Default model\n",
        "                     task = \"text-generation\")\n",
        "\n",
        "generator(\"These are some steps for build risk forecast model\",\n",
        "          max_new_tokens = 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yw8eATWnjok",
        "outputId": "90a0fccd-bf8f-44c3-fff6-3a50e87f6dfe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"These are some steps for build risk forecast model (SSM) and prediction system (PWS) that make the best use of this information.\\n\\n1. To forecast forecast performance based on the results of a particular investment.\\n\\nThis is a very important idea. It's what the\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 文本生成\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "\n",
        "generator(\n",
        "    \"These are some steps for build risk forecast model\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLCJXBWKsMJF",
        "outputId": "9e06742d-0c0e-4013-ea38-d8c1d4aad921"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'These are some steps for build risk forecast model design.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'},\n",
              " {'generated_text': 'These are some steps for build risk forecast model research. For more information, visit our\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **== 1. Transformer ==**"
      ],
      "metadata": {
        "id": "6w95AXYX77De"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Pipeline"
      ],
      "metadata": {
        "id": "sXngvY3RB2m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(model = \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "                      task = \"sentiment-analysis\")\n",
        "\n",
        "classifier(\n",
        "    [\n",
        "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "        \"I hate this so much!\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3gi8KyN8MCi",
        "outputId": "2a337a77-2f97-4309-b411-e883f5280a06"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.1 Tokenizer"
      ],
      "metadata": {
        "id": "IUGEv06fB9lX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 使用預訓練過的 checkpoint\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "LPukztIu8t-o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CheckTokenize\n",
        "\n",
        "raw_inputs = [\"I've been waiting for a HuggingFace course my whole life.\",\n",
        "              \"I hate this so much!\",]\n",
        "\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\") # 將返回 dict\n",
        "\n",
        "print(inputs[\"input_ids\"])\n",
        "print(inputs[\"attention_mask\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swa9y9mh9ClX",
        "outputId": "93cf92f7-4e67-4eed-fd38-21062741f087"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
            "          2607,  2026,  2878,  2166,  1012,   102],\n",
            "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0]])\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.2 Through pretrained model"
      ],
      "metadata": {
        "id": "pzMGTvmyCOKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "9ZBzL9nX9r8-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model's output\n",
        "outputs = model(**inputs)\n",
        "\n",
        "outputs.logits\n",
        "\n",
        "# 第一句, 負面情緒的機率, 正面情緒的機率\n",
        "# 第二句, 負面情緒的機率, 正面情緒的機率"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pSPLYSt_2E9",
        "outputId": "cf9e9e64-45b0-4145-c840-233a0168aeda"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.5607,  1.6123],\n",
              "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1.3 Logits -> 有意義的回答"
      ],
      "metadata": {
        "id": "JxDTqvVvGd89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 情緒字典\n",
        "class_names = model.config.id2label\n",
        "\n",
        "# logits -> probs -> label_index\n",
        "probility = torch.softmax(outputs.logits, dim = 1)\n",
        "label = torch.argmax(probility, dim = 1)\n",
        "\n",
        "# 第一句\n",
        "print(f\"第一句情緒判斷:{class_names[label[0].item()]}\")\n",
        "# 第二句\n",
        "print(f\"第二句情緒判斷:{class_names[label[1].item()]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyqjG6aPDLFu",
        "outputId": "a86837cb-bd5a-4147-a3a2-8b73539d97ea"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第一句情緒判斷:POSITIVE\n",
            "第二句情緒判斷:NEGATIVE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Model"
      ],
      "metadata": {
        "id": "yjRFnLYvGztz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.1 Get pretrained model"
      ],
      "metadata": {
        "id": "rQKchKarHAhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "# 使用此模型作者提供的 checkpoint\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "# [INFO] -> 如果需要客製化, 需要整定參數"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8V-LLtiHEax",
        "outputId": "09be7125-f785-416f-d0e6-8e70956757de"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2.2 Save model"
      ],
      "metadata": {
        "id": "tAAxpCwvI34V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"Model_of_Bert\")\n",
        "\n",
        "# 將於指定資料夾名稱中, 儲存兩個文件\n",
        "# 1. config.json  模型屬性\n",
        "# 2. pytorch_model.bin  模型的權重"
      ],
      "metadata": {
        "id": "FaLiF0_DIycS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Tokenizer\n",
        "句子 -> 數字\n",
        "* Word-based\n",
        "* Character-based (對英文較無意義, 因為英文通常一個字就是一個意思 / 對中文意義較大)\n",
        "* Save tokenizer"
      ],
      "metadata": {
        "id": "QvJ38UlbJUQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.1 Word-based"
      ],
      "metadata": {
        "id": "zfHgsB0PMxNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What is we have seven days for weekend?\"\n",
        "\n",
        "tokenized_text = text.split()\n",
        "tokenized_text\n",
        "\n",
        "# 0 -> What\n",
        "# 1 -> is\n",
        "# ...\n",
        "# 8 -> unknown"
      ],
      "metadata": {
        "id": "OKSO08ktMw4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c14ef25-2dda-4172-c30e-8485bd25beb7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What', 'is', 'we', 'have', 'seven', 'days', 'for', 'weekend?']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.2 Pretrained tokenizer"
      ],
      "metadata": {
        "id": "1OIh41LDPyT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# # 以下同效果\n",
        "# from transformers import AutoTokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "rcWu-NO3Px4o"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.3 Save tokenizer"
      ],
      "metadata": {
        "id": "q6kMjU9VM2M4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"Toeknizer_of_Bert\")"
      ],
      "metadata": {
        "id": "FNr64V4SM2ia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76b6f658-1774-44a2-880a-c174dc38bc9c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Toeknizer_of_Bert/tokenizer_config.json',\n",
              " 'Toeknizer_of_Bert/special_tokens_map.json',\n",
              " 'Toeknizer_of_Bert/vocab.txt',\n",
              " 'Toeknizer_of_Bert/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3.4 Decode"
      ],
      "metadata": {
        "id": "ajJf7RnOTd3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Today is Sunday\"\n",
        "print(f\"Original text: {text}\")\n",
        "\n",
        "token = tokenizer(text)\n",
        "print(f\"Encode text: {token['input_ids']}\")\n",
        "\n",
        "untoken = tokenizer.decode(token['input_ids'])\n",
        "print(f\"Decode token: {untoken}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gCyfyACTGhk",
        "outputId": "13483490-53bb-4d47-b143-64185bb12f18"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original text: Today is Sunday\n",
            "Encode text: [101, 3570, 1110, 3625, 102]\n",
            "Decode token: [CLS] Today is Sunday [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Tokenizer 如何處理多個序列"
      ],
      "metadata": {
        "id": "6gvU-EoaRLPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.1"
      ],
      "metadata": {
        "id": "hiJm0yxuVP6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Auto 將會自動根據 checkpoint 找尋 tokenizer & model for Sequence Classification\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)                       # will return dictionary\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)      #\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "token = tokenizer(sequence, return_tensors=\"pt\")\n",
        "token = token[\"input_ids\"].squeeze(dim=1)\n",
        "\n",
        "model(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ1GGG8-VPoP",
        "outputId": "3871ed6d-f468-431d-f123-347074770daa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SequenceClassifierOutput(loss=None, logits=tensor([[-1.5607,  1.6123]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4.2"
      ],
      "metadata": {
        "id": "Gx5h-0jdVR91"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OP2QC3sdVTGv"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}